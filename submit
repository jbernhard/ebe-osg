#!/bin/bash

set -e


#
# settings
#

# max run time (seconds)
MAX_WALL_TIME=171000    # 47.5 hours

# megabytes
MEMORY_REQUIREMENT=1024

# gridftp destination
# files will be placed in "$DEST_URL/$RUN_ID"
# take value from environment or fall back to default
DEST_URL=${DEST_URL:-gsiftp://ntheoryfs01.phy.duke.edu/var/phy/project/nukeserv/$USER/ebe-events}


#
# initialization
#

# load output functions
source lib/msg

msg 'EbE-OSG job generator and submitter'

# check that first argument is a positive integer
# otherwise print usage information
if ! (( $1 > 0 )) 2> /dev/null; then
  cat <<EOF
usage:  $0 number_of_jobs [input_files ...]

required:
  number_of_jobs = positive integer

optional:
  input_files = relative path[s] to input file[s]

The GridFTP destination may be overriden by setting it as an environment variable.  The current value is
  DEST_URL=$DEST_URL

This script automatically creates a new grid-proxy if necessary.  For convenience, the grid-password will be read 
from the file .gridpw in this directory.  If the file is not present, the password will be requested normally.
EOF
  exit 1
fi

# first argument is number of jobs to submit
NUM_RUNS=$1

# remaining arguments are input files
shift 
INPUT_FILES=$@

for F in ${INPUT_FILES[@]}; do
  INPUT_FILES_JOINED="${INPUT_FILES_JOINED}_`basename $F`"
done

# init proxy if not enough time remains
echo
if grid-proxy-info -exists -valid $(( $MAX_WALL_TIME / 3600 * 3 )):00 2> /dev/null; then
  msg 'grid-proxy is valid'
else
  warning "grid-proxy too old or doesn't exist, creating a new one"
  if [[ -f .gridpw ]];  then
    msg2 'reading password from .gridpw'

    perm=$(stat -c %a .gridpw)
    [[ "$perm" != '600' ]] && {
      warning ".gridpw permissions are $perm (`stat -c %A .gridpw`), recommended permissions are 600 (-rw-------)"
      read -p 'chmod 600 .gridpw? [y/N] ' ch
      [[ "$ch" == 'y' ]] && chmod -v 600 .gridpw
    }

    grid-proxy-init -valid $(( $MAX_WALL_TIME / 3600 * 4 )):00 -pwstdin < .gridpw
  else
    msg2 'file .gridpw does not exist, requesting password input'
    grid-proxy-init -valid $(( $MAX_WALL_TIME / 3600 * 4 )):00
  fi
fi

# top dir
TOP_DIR=$PWD

# gridftp source location
# engage server + cwd
SRC_URL=gsiftp://`hostname -f`"$TOP_DIR"



#
# generate jobs
#

echo
msg "generating $NUM_RUNS jobs for input file[s] ${INPUT_FILES[@]}"

# generate run id
RUN_ID="`/bin/date +'%F_%H%M%S'`_${NUM_RUNS}jobs${INPUT_FILES_JOINED}"
msg2 "run ID:  $RUN_ID"

# create run dir and subfolders
RUN_DIR=$TOP_DIR/runs/$RUN_ID
mkdir -p $RUN_DIR/{condor,stdouterr,dag}
touch $RUN_DIR/alljobs.log
chmod 644 $RUN_DIR/alljobs.log

USER_ID=`id -u`

for (( JOB_ID=0; JOB_ID<$NUM_RUNS; JOB_ID++ )); do
  # condor submit file
  cd $RUN_DIR
  cat > condor/$JOB_ID <<EOF
universe        = vanilla

# Specify voms proxy here
x509userproxy   = /tmp/x509up_u$USER_ID

# requirements is an expression to specify machines that can run jobs
#requirements    = ( Memory >= $MEMORY_REQUIREMENT && OpSys == "LINUX" ) && ( Arch == "INTEL" || Arch == "X86_64" ) && !regexp("acas[0-9]+.usatlas.bnl.gov", Machine)
#requirements    = ( Memory >= $MEMORY_REQUIREMENT && OpSys == "LINUX" ) && ( Arch == "INTEL" || Arch == "X86_64" ) && regexp("acas[0-9]+.usatlas.bnl.gov", Machine)
requirements    = ( GLIDEIN_Max_Walltime >= $MAX_WALL_TIME && Memory >= $MEMORY_REQUIREMENT && OpSys == "LINUX" ) && ( Arch == "INTEL" || Arch == "X86_64" )

# make sure the job is being retried and rematched
periodic_release = (NumGlobusSubmits < 5) && ((CurrentTime - EnteredCurrentStatus) > (60*60))

# protect against hung jobs
periodic_hold =  (JobStatus==2) && ((CurrentTime - EnteredCurrentStatus) > ($MAX_WALL_TIME))

# stay in queue on failures
on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

executable = ../../lib/remote-job-wrapper
arguments = $RUN_ID $JOB_ID $SRC_URL $DEST_URL ${INPUT_FILES[@]}

should_transfer_files = YES
WhenToTransferOutput = ON_EXIT

output = stdouterr/$JOB_ID.out
error = stdouterr/$JOB_ID.err
log = alljobs.log

notification = NEVER

queue
EOF

  # update dag
  echo "" >> dag/master.dag
  echo "JOB    job_$JOB_ID condor/$JOB_ID" >> dag/master
  echo "RETRY  job_$JOB_ID 3" >> dag/master
done

echo
msg 'submitting jobs'
condor_submit_dag -notification NEVER -maxidle 200 dag/master
